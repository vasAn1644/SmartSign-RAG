{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93466337",
   "metadata": {},
   "source": [
    "# RAG System for German Road Signs\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline for German traffic signs, including both textual and visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6bd1f",
   "metadata": {},
   "source": [
    "## 1. Image Parsing and JSON Metadata Generation\n",
    "- Extract images from [iamexpat.de road signs section](https://www.iamexpat.de/expat-info/driving-germany/road-signs)\n",
    "- Generate structured JSON metadata containing:\n",
    "  - Image URL\n",
    "  - Sign description\n",
    "  - Sign category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81653ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to https://www.iamexpat.de/expat-info/driving-germany/road-signs...\n",
      "300  road signs collected\n",
      "0 need manual description\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "URL = \"https://www.iamexpat.de/expat-info/driving-germany/road-signs\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def parse_all_signs():\n",
    "    print(f\"Connecting to {URL}...\")\n",
    "    response = requests.get(URL, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error loading page\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    all_signs_data = []\n",
    "    \n",
    "    content_area = soup.find('div', class_='article__content') or soup.find('body')\n",
    "    \n",
    "    all_imgs = content_area.find_all('img')\n",
    "\n",
    "    for img in all_imgs:\n",
    "        src = img.get('src', '')\n",
    "        \n",
    "        if 'road-sign' not in src.lower() and 'sign' not in src.lower() and not src.endswith('.svg'):\n",
    "            if not img.get('alt'): \n",
    "                continue\n",
    "\n",
    "        img_url = src if src.startswith('http') else \"https://www.iamexpat.de\" + src\n",
    "        \n",
    "\n",
    "        title = img.get('alt', '').strip() or img.get('title', '').strip()\n",
    "        \n",
    "        if not title:\n",
    "            parent_td = img.find_parent('td')\n",
    "            if parent_td:\n",
    "                title = parent_td.get_text(strip=True)\n",
    "                if not title and parent_td.find_next_sibling('td'):\n",
    "                    title = parent_td.find_next_sibling('td').get_text(strip=True)\n",
    "\n",
    "\n",
    "        if not title or len(title) < 2:\n",
    "            title = \"NEED_MANUAL_DESCRIPTION\"\n",
    "\n",
    "        category = \"General\"\n",
    "        prev_h = img.find_previous(['h2', 'h3'])\n",
    "        if prev_h:\n",
    "            category = prev_h.get_text(strip=True)\n",
    "\n",
    "        all_signs_data.append({\n",
    "            \"category\": category,\n",
    "            \"title\": title,\n",
    "            \"image_url\": img_url,\n",
    "            \"status\": \"manual_check\" if title == \"NEED_MANUAL_DESCRIPTION\" else \"ok\"\n",
    "        })\n",
    "\n",
    "    unique_data = {item['image_url']: item for item in all_signs_data}.values()\n",
    "\n",
    "    with open(\"data/germany_road_signs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(unique_data), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"{len(unique_data)}  road signs collected\")\n",
    "    manual_count = sum(1 for x in unique_data if x['status'] == 'manual_check')\n",
    "    print(f\"{manual_count} need manual description\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse_all_signs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae313789",
   "metadata": {},
   "source": [
    "## 2. Text Data Loading\n",
    "- Load textual information from the road signs section\n",
    "- Prepare documents for downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace5f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/road-signs\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/driving-licence\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/learning-to-drive\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/buying-a-car\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/car-leasing\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/registering-vehicle\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/motor-vehicle-tax\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/emissions-sticker\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/periodic-technical-inspection-hauptuntersuchung-tuev\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/importing-car\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/exporting-car\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/german-autobahn\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/traffic-fines\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/car-insurance\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/importing-exporting-vehicles\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/motor-vehicle-tax-emissions-badges\n",
      "Processing: https://www.iamexpat.de/expat-info/driving-germany/learning-how-to-drive\n",
      "\n",
      "Texts saved: 18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "START_URLS = [\n",
    "    \"https://www.iamexpat.de/expat-info/driving-germany/road-signs\",\n",
    "    \"https://www.iamexpat.de/expat-info/driving-germany\"\n",
    "]\n",
    "\n",
    "BASE_DOMAIN = \"www.iamexpat.de\"\n",
    "OUTPUT_DIR = \"data/text_files\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "visited = set()\n",
    "\n",
    "\n",
    "def clean_filename(url: str) -> str:\n",
    "    path = urlparse(url).path.strip(\"/\")\n",
    "    if not path:\n",
    "        return \"index.txt\"\n",
    "    name = path.split(\"/\")[-1]\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", name) + \".txt\"\n",
    "\n",
    "\n",
    "def extract_text(soup: BeautifulSoup) -> str:\n",
    "    content = []\n",
    "\n",
    "    for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"]):\n",
    "        text = tag.get_text(\" \", strip=True)\n",
    "        if len(text) > 30:\n",
    "            content.append(text)\n",
    "\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "\n",
    "def is_valid_link(link: str) -> bool:\n",
    "    parsed = urlparse(link)\n",
    "    return (\n",
    "        parsed.netloc == BASE_DOMAIN\n",
    "        and parsed.path.startswith(\"/expat-info/driving-germany\")\n",
    "    )\n",
    "\n",
    "\n",
    "def process_page(url: str):\n",
    "    if url in visited:\n",
    "        return\n",
    "\n",
    "    print(f\"Processing: {url}\")\n",
    "    visited.add(url)\n",
    "\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "\n",
    "    text = extract_text(soup)\n",
    "    if text:\n",
    "        filename = clean_filename(url)\n",
    "        filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        link = urljoin(url, a[\"href\"])\n",
    "        if is_valid_link(link):\n",
    "            process_page(link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for start_url in START_URLS:\n",
    "    process_page(start_url)\n",
    "\n",
    "print(f\"\\nTexts saved: {len(os.listdir(OUTPUT_DIR))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ed297",
   "metadata": {},
   "source": [
    "## 3. Data Chunking\n",
    "- Split all documents (image metadata + textual data) into manageable chunks\n",
    "- Ensure metadata is preserved for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eda4f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\RAG test project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 18/18 [00:00<00:00, 576.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents:\n",
      "- Image JSON docs: 300\n",
      "- Text docs: 18\n",
      "- TOTAL: 318\n",
      "\n",
      "Created 633 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def load_image_json(json_path: str) -> List[Document]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    for item in data:\n",
    "        title = item.get(\"title\", \"\").strip()\n",
    "        category = item.get(\"category\", \"\").strip()\n",
    "        image_url = item.get(\"image_url\", \"\").strip()\n",
    "\n",
    "        page_content = f\"Traffic sign: {title}. Category: {category}.\"\n",
    "\n",
    "        metadata = {\n",
    "            \"type\": \"image\",\n",
    "            \"title\": title,\n",
    "            \"category\": category,\n",
    "            \"image_url\": image_url\n",
    "        }\n",
    "\n",
    "        documents.append(\n",
    "            Document(\n",
    "                page_content=page_content,\n",
    "                metadata=metadata\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "def load_text_files(path: str) -> List[Document]:\n",
    "    loader = DirectoryLoader(\n",
    "        path,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "        show_progress=True\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "\n",
    "class UniversalChunker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50\n",
    "    ):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "    def chunk(self, documents: List[Document]) -> List[Document]:\n",
    "        return self.splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "image_docs = load_image_json(\"data/germany_road_signs.json\")\n",
    "text_docs = load_text_files(\"data/text_files\")\n",
    "\n",
    "all_documents = image_docs + text_docs\n",
    "\n",
    "print(f\"Loaded documents:\")\n",
    "print(f\"- Image JSON docs: {len(image_docs)}\")\n",
    "print(f\"- Text docs: {len(text_docs)}\")\n",
    "print(f\"- TOTAL: {len(all_documents)}\")\n",
    "\n",
    "chunker = UniversalChunker()\n",
    "chunks = chunker.chunk(all_documents)\n",
    "\n",
    "print(f\"\\nCreated {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc84e0",
   "metadata": {},
   "source": [
    "## 4. Embeddings Creation and Vector Storage\n",
    "- Convert chunks to vector embeddings using the `sentence-transformers/all-MiniLM-L12-v2` model\n",
    "- Persist embeddings in a Chroma vector store for semantic retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21a91ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_13100\\1292074488.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to embed: 633\n",
      "Image chunks: 300\n",
      "Text chunks: 333\n",
      " Vector store created\n",
      "Stored embeddings: 633\n",
      "\n",
      "Result 1\n",
      "TEXT: Traffic sign: Slippery When Wet. Category: Traffic signs in Germany: Warning signs.\n",
      "METADATA: {'type': 'image', 'category': 'Traffic signs in Germany: Warning signs', 'image_url': 'https://iamexpat.directus.app/assets/f1f65762-e9f7-45ed-a274-af7a9b15d05b?width=120&height=105', 'title': 'Slippery When Wet'}\n",
      "\n",
      "Result 2\n",
      "TEXT: Traffic sign: Oil Slick. Category: Supplementary signs (Zusatzschilder).\n",
      "METADATA: {'image_url': 'https://iamexpat.directus.app/assets/551a49e7-7fbd-46e0-9681-b89e991462c7?width=120&height=66', 'title': 'Oil Slick', 'type': 'image', 'category': 'Supplementary signs (Zusatzschilder)'}\n",
      "\n",
      "Result 3\n",
      "TEXT: Traffic sign: parking on pavement allowed wholly. Category: Parking signs.\n",
      "METADATA: {'title': 'parking on pavement allowed wholly', 'category': 'Parking signs', 'image_url': 'https://iamexpat.directus.app/assets/30336666-995c-441c-b9bf-168e3b49fe3f', 'type': 'image'}\n",
      "\n",
      "Result 4\n",
      "TEXT: Traffic sign: yield. Category: Right-of-way signs.\n",
      "METADATA: {'title': 'yield', 'type': 'image', 'category': 'Right-of-way signs', 'image_url': 'https://iamexpat.directus.app/assets/1e57e2c6-e312-4fdb-81a9-f62745427f0a'}\n",
      "\n",
      "Result 5\n",
      "TEXT: Traffic sign: parking on pavement allowed half. Category: Parking signs.\n",
      "METADATA: {'image_url': 'https://iamexpat.directus.app/assets/077f9f13-9adc-4e92-87c9-1ad7acc683c1', 'type': 'image', 'title': 'parking on pavement allowed half', 'category': 'Parking signs'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "print(f\"Total chunks to embed: {len(chunks)}\")\n",
    "\n",
    "image_chunks = [c for c in chunks if c.metadata.get(\"type\") == \"image\"]\n",
    "text_chunks = [c for c in chunks if c.metadata.get(\"type\") != \"image\"]\n",
    "\n",
    "print(f\"Image chunks: {len(image_chunks)}\")\n",
    "print(f\"Text chunks: {len(text_chunks)}\")\n",
    "\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,              \n",
    "    embedding=embedding_model,\n",
    "    persist_directory=PERSIST_DIR\n",
    ")\n",
    "\n",
    "print(\" Vector store created\")\n",
    "print(\"Stored embeddings:\", vectorstore._collection.count())\n",
    "\n",
    "results = vectorstore.similarity_search(\"slippery road\", k=5)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}\")\n",
    "    print(\"TEXT:\", doc.page_content)\n",
    "    print(\"METADATA:\", doc.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9905f5d",
   "metadata": {},
   "source": [
    "## 5. AI Model Integration\n",
    "- Initialize a language model (LLM) for RAG\n",
    "- Connect the LLM with a retriever to query vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9817c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AIML_API_KEY = os.getenv(\"AIML_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    api_key=AIML_API_KEY,\n",
    "    base_url=\"https://api.aimlapi.com/v1\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted = []\n",
    "    for d in docs:\n",
    "        block = f\"\"\"\n",
    "TEXT:\n",
    "{d.page_content}\n",
    "\n",
    "IMAGE_URL:\n",
    "{d.metadata.get(\"image_url\", \"None\")}\n",
    "\n",
    "CATEGORY:\n",
    "{d.metadata.get(\"category\", \"Unknown\")}\n",
    "\"\"\"\n",
    "        formatted.append(block.strip())\n",
    "    return \"\\n\\n---\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant on German features of road rules.\n",
    "\n",
    "Use the provided context to answer the question.\n",
    "If the context contains image metadata, mention the image(s) when relevant.\n",
    "\n",
    "If the description of a sign is incomplete, you MAY infer its meaning\n",
    "based on common traffic rules and the sign category.\n",
    "Do NOT invent specific legal details that are not implied by the context.\n",
    "\n",
    "When images are relevant, include them in your answer using this format:\n",
    "\n",
    "Image: <image_url>\n",
    "Explanation: <short explanation of the sign>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a clear and structured way.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47640716",
   "metadata": {},
   "source": [
    "## 6. RAG Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcfec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slippery road traffic sign, often represented as an oil slick sign, indicates that the road surface may be slippery due to oil, rain, or other conditions. This sign is a supplementary sign (Zusatzschild) that warns drivers to exercise caution and reduce speed to prevent accidents.\n",
      "\n",
      "Image: ![Oil Slick Sign](https://iamexpat.directus.app/assets/551a49e7-7fbd-46e0-9681-b89e991462c7?width=120&height=66)  \n",
      "Explanation: The sign alerts drivers to potential slippery conditions on the road.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What does a slippery road traffic sign mean?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496eeb99",
   "metadata": {},
   "source": [
    "### перепвірка по семантичному пошуку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e5583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Traffic sign: Slippery When Wet. Category: Traffic signs in Germany: Warning signs.\n",
      "IMAGE: https://iamexpat.directus.app/assets/f1f65762-e9f7-45ed-a274-af7a9b15d05b?width=120&height=105\n",
      "TEXT: Traffic sign: Oil Slick. Category: Supplementary signs (Zusatzschilder).\n",
      "IMAGE: https://iamexpat.directus.app/assets/551a49e7-7fbd-46e0-9681-b89e991462c7?width=120&height=66\n",
      "TEXT: Traffic sign: parking on pavement allowed wholly. Category: Parking signs.\n",
      "IMAGE: https://iamexpat.directus.app/assets/30336666-995c-441c-b9bf-168e3b49fe3f\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(\"slippery road\", k=3)\n",
    "\n",
    "for d in docs:\n",
    "    print(\"TEXT:\", d.page_content)\n",
    "    print(\"IMAGE:\", d.metadata.get(\"image_url\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG test project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
